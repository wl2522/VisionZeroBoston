{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Modeling Text Data\n",
    "\n",
    "### Team Member 1:\n",
    "* UNI:  WL2522\n",
    "* Name: Wilson Lui\n",
    "\n",
    "### Team Member 2 [optional]:\n",
    "* UNI:  \n",
    "* Name:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the data here: https://data.boston.gov/dataset/vision-zero-entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 - Data Cleaning  [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, visualize the class distribution. Clean up the target labels. Some categories have been arbitrarily split and need to be consolidated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, rows with no comments or duplicate rows that have the same combination of request type and comment are removed. Rows with  comments that are less than 12 characters long were also removed due to comments of that length not being too informative.\n",
    "\n",
    "\n",
    "Then, embedded image tags are removed from the categories.\n",
    "\n",
    "Having removed the embedded image tags from the categories, the following categories are consolidated due to them having the same meaning with different wordings:\n",
    "\n",
    "\n",
    "1. \"bike facilities don't exist or need improvement\" / \"there are no bike facilities or they need maintenance\"\n",
    "\n",
    "2. \"sidewalks/ramps don't exist or need improvement\" / \"there are no sidewalks or they need maintenance\"\n",
    "\n",
    "3. \"the wait for the \"Walk\" signal is too long\" / \"people have to wait too long for the \"Walk\" signal\"\n",
    "\n",
    "4. \"the roadway surface needs improvement\" / \"the roadway surface needs maintenance\"\n",
    "\n",
    "5. \"it’s hard to see / low visibility\" / \"it’s hard for people to see each other\"\n",
    "\n",
    "6. \"it's too far / too many lanes to cross\" / \"people have to cross too many lanes / too far\"\n",
    "\n",
    "7. \"people are not given enough time to cross the street\" / \"there's not enough time to cross the street\"\n",
    "\n",
    "The following categories are very similar to each other, but not quite the same, and thus were not combined during this preprocessing step:\n",
    "\n",
    "\n",
    ">\"people don't yield while going straight\" / \"people don't yield while turning\" /  \"people run red lights / stop signs\"\n",
    "\n",
    "\n",
    "Whenever categories are consolidated, I merge the category with fewer data points into the one with more data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:48.391662",
     "start_time": "2017-04-16T12:40:48.339612"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "data = pd.read_csv('Vision_Zero_Entry.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:48.771073",
     "start_time": "2017-04-16T12:40:48.393165"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the distribution of the categories\n",
    "\n",
    "\n",
    "print(data['REQUESTTYPE'].unique())\n",
    "plt.barh(range(len(data['REQUESTTYPE'].unique())), list(Counter(data['REQUESTTYPE']).values()), align='center')\n",
    "plt.yticks(range(len(data['REQUESTTYPE'].unique())), list(Counter(data['REQUESTTYPE']).keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:48.896192",
     "start_time": "2017-04-16T12:40:48.772574"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove rows with no comments\n",
    "#Remove duplicate rows that have the same category and comment\n",
    "#Remove rows with comments that are less than 12 characters long\n",
    "#Create a list of categories\n",
    "#Consolidate the categories that have embedded images tags into corresponding categories\n",
    "\n",
    "\n",
    "data = data[data['COMMENTS'].notnull()]\n",
    "data = data.drop_duplicates(subset=['REQUESTTYPE', 'COMMENTS'])\n",
    "data = data[data['COMMENTS'].str.len() > 12]\n",
    "\n",
    "\n",
    "print(len(data['REQUESTTYPE'].unique()))\n",
    "categories = data['REQUESTTYPE'].unique()\n",
    "categories = categories.tolist()\n",
    "categories.sort()\n",
    "\n",
    "for a in range(len(categories)): print(categories[a])\n",
    "    \n",
    "data['REQUESTTYPE'].replace(to_replace=categories[0:7], value=[\n",
    "        \"there's not enough time to cross the street\",\n",
    "    'the wait for the \"Walk\" signal is too long',\n",
    "    'people speed', 'it’s hard to see / low visibility',\n",
    "        \"sidewalks/ramps don't exist or need improvement\", \"the roadway surface needs improvement\",\n",
    "    \"of something that is not listed here\"], inplace=True)\n",
    "\n",
    "print(data['REQUESTTYPE'].unique())\n",
    "print(len(data['REQUESTTYPE'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:49.184504",
     "start_time": "2017-04-16T12:40:48.897194"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the distribution of the categories after the first round of category consolidation\n",
    "\n",
    "\n",
    "print(data['REQUESTTYPE'].unique())\n",
    "plt.barh(range(len(data['REQUESTTYPE'].unique())), list(Counter(data['REQUESTTYPE']).values()), align='center')\n",
    "plt.yticks(range(len(data['REQUESTTYPE'].unique())), list(Counter(data['REQUESTTYPE']).keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:49.202007",
     "start_time": "2017-04-16T12:40:49.185492"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Count how many occurrences of each category occur in the dataset\n",
    "#Create a list with tuples containing each category and its number of occurrences\n",
    "\n",
    "\n",
    "count = Counter(data['REQUESTTYPE'])\n",
    "count = count.items()\n",
    "count = list(count)\n",
    "count.sort()\n",
    "print(count)\n",
    "\n",
    "print(count[14][0], count[19][0])\n",
    "print(count[0][0], count[18][0])\n",
    "print(count[11][0], count[17][0])\n",
    "print(count[15][0], count[16][0])\n",
    "print(count[2][0], count[3][0])\n",
    "print(count[1][0], count[10][0])\n",
    "print(count[5][0], count[20][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:49.273076",
     "start_time": "2017-04-16T12:40:49.203509"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For each category pair, check which category appears less often\n",
    "#Replace that category with the other category\n",
    "\n",
    "\n",
    "#bike facilities don't exist or need improvement / there are no bike facilities or they need maintenance\n",
    "\n",
    "\n",
    "if count[0][1] > count[18][1]: \n",
    "    data['REQUESTTYPE'].replace(to_replace=count[18][0], value=count[0][0], inplace=True)\n",
    "\n",
    "elif count[18][1] > count[0][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[0][0], value=count[18][0], inplace=True)\n",
    "\n",
    "    \n",
    "#sidewalks/ramps don't exist or need improvement / there are no sidewalks or they need maintenance\n",
    "\n",
    "\n",
    "if count[14][1] > count[19][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[19][0], value=count[14][0], inplace=True)\n",
    "    \n",
    "elif count[19][1] > count[14][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[14][0], value=count[19][0], inplace=True)\n",
    "    \n",
    "    \n",
    "#people have to wait too long for the \"Walk\" signal / the wait for the \"Walk\" signal is too long\n",
    "\n",
    "\n",
    "if count[11][1] > count[17][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[17][0], value=count[11][0], inplace=True)\n",
    "    \n",
    "elif count[17][1] > count[11][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[11][0], value=count[17][0], inplace=True)\n",
    "    \n",
    "\n",
    "#the roadway surface needs improvement / the roadway surface needs maintenance\n",
    "\n",
    "\n",
    "if count[15][1] > count[16][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[16][0], value=count[15][0], inplace=True)\n",
    "    \n",
    "elif count[16][1] > count[15][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[15][0], value=count[16][0], inplace=True)\n",
    "    \n",
    "    \n",
    "#it’s hard to see / low visibility / it’s hard for people to see each other\n",
    "\n",
    "\n",
    "if count[2][1] > count[3][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[3][0], value=count[2][0], inplace=True)\n",
    "    \n",
    "elif count[3][1] > count[2][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[2][0], value=count[3][0], inplace=True)\n",
    "    \n",
    "    \n",
    "#it's too far / too many lanes to cross / people have to cross too many lanes / too far\n",
    "\n",
    "\n",
    "if count[1][1] > count[10][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[10][0], value=count[1][0], inplace=True)\n",
    "    \n",
    "elif count[10][1] > count[1][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace[1][0], value=count[10][0], inplace=True)\n",
    "\n",
    "    \n",
    "#people are not given enough time to cross the street / there's not enough time to cross the street\n",
    "\n",
    "\n",
    "if count[5][1] > count[20][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[20][0], value=count[5][0], inplace=True)\n",
    "    \n",
    "elif count[20][1] > count[5][1]:\n",
    "    data['REQUESTTYPE'].replace(to_replace=count[5][0], value=count[20][0], inplace=True)\n",
    "\n",
    "print(Counter(data['REQUESTTYPE']))\n",
    "print(len(data['REQUESTTYPE'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:49.284587",
     "start_time": "2017-04-16T12:40:49.274576"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a copy of the dataset\n",
    "#Separate the features from the response variable\n",
    "\n",
    "consolidated_data = data.copy()\n",
    "\n",
    "target = data['REQUESTTYPE']\n",
    "comments = data['COMMENTS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:49.523816",
     "start_time": "2017-04-16T12:40:49.285588"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the distribution of the consolidated categories\n",
    "\n",
    "\n",
    "print(target.unique())\n",
    "plt.barh(range(len(target.unique())), list(Counter(target).values()), align='center')\n",
    "plt.yticks(range(len(target.unique())), list(Counter(target).keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2 - Model 1 [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a baseline multi-class classification model using a bag-of-word approach, report macro f1-score (should be above .5) and visualize the confusion matrix. Can you interpret the mistakes made by the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original selection of 28 categories have now been consolidated into the following 14:\n",
    "\n",
    "\n",
    "1. \"bike facilities don't exist or need improvement\"\n",
    "\n",
    "2. \"of something that is not listed here\"\n",
    "\n",
    "3. \"people don't yield while going straight\"\n",
    "\n",
    "4. \"it’s hard to see / low visibility\"\n",
    "\n",
    "5. \"people don't yield while turning\"\n",
    "\n",
    "6. \"the wait for the \"Walk\" signal is too long\"\n",
    "\n",
    "7. \"sidewalks/ramps don't exist or need improvement\"\n",
    "\n",
    "8. \"people cross away from the crosswalks\"\n",
    "\n",
    "9. \"people double park their vehicles\"\n",
    "\n",
    "10. \"people speed\"\n",
    "\n",
    "11. \"people run red lights / stop signs\"\n",
    "\n",
    "12. \"it's too far / too many lanes to cross\"\n",
    "\n",
    "13. \"there's not enough time to cross the street\"\n",
    "\n",
    "14. \"the roadway surface needs improvement\"\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T12:40:49.690490",
     "start_time": "2017-04-16T12:40:49.524817"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the dataset into training and test sets\n",
    "#Vectorize the comments\n",
    "\n",
    "\n",
    "comments_train, comments_test, target_train, target_test = train_test_split(comments,\n",
    "                                                                            target, stratify=target,\n",
    "                                                                           random_state=3)\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "X_train = vect.fit_transform(comments_train)\n",
    "X_test = vect.transform(comments_test)\n",
    "\n",
    "#X_train_scaled = reg.fit_transform(X_train)\n",
    "#X_test_scaled = reg.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T21:09:31.891529",
     "start_time": "2017-04-15T21:09:27.368071"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score a baseline multinomial naive Bayes model using F1-macro score\n",
    "\n",
    "\n",
    "baseline = np.mean(cross_val_score(LogisticRegression(random_state=3), X_train, target_train,\n",
    "                                   cv=5, scoring='f1_macro', n_jobs=8))\n",
    "print('Baseline F1 Macro score:', baseline)\n",
    "\n",
    "assert baseline > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T21:31:40.292490",
     "start_time": "2017-04-15T21:31:35.227577"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Display the confusion matrix and classification report\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state=3)\n",
    "lr.fit(X_train, target_train)\n",
    "\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "baseline_preds = cross_val_predict(lr, X_train, target_train, cv=5, n_jobs=8)\n",
    "\n",
    "print(confusion_matrix(target_train, baseline_preds))\n",
    "print(classification_report(target_train, baseline_preds))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix and classification report, it appears that model is having trouble with these particular aspects of the model:\n",
    "\n",
    "1. There are a great number of data points that are incorrectly classified as \"of something that is not listed here\". There are also many data points belonging to that category but incorrectly classified as other categories. Browsing through the comments belonging to \"of something that is not listed here\", it seems that many should belong to categories that already exist. Without having looked at the interface through which these complaints were collected, I would guess that maybe \"of something that is not listed here\" is the default option and many people don't bother changing it before submitting their complaints\n",
    "\n",
    "2. The similarities between comments in the group \"people don't yield while going straight\", \"people don't yield while turning\", and \"people run red lights / stop signs\" are confusing the model. In the confusion matrix, it appears that a portion of the data points in each category are being mistakenly classified as other categories in this group.\n",
    "\n",
    "3. Some data points in the \"people speed\" and \"people run red lights / stop signs\" categories are being mistakenly categorized as the other category. Similar to #1, many other data points belonging to other categories are being mistakenly categorized as one of these two categories.\n",
    "\n",
    "4. Some data points in the group \"bike facilities don't exist or need improvement\", \"sidewalks/ramps don't exist or need improvement\", and \"the roadway surface needs improvement\" categories are being mistakenly categorized as other categories in this group. This seems to be due to the fact that complaints about bike facilities include those about poor road conditions or faded lane marking paint in bike lanes, which would overlap with comments about sidewalks and roadway conditions.\n",
    "\n",
    "5. To a lesser extent than in #1, many data points are being mistakenly classified as \"bike facilities don't exist or need improvement\", most likely due to complaints about the bike facilities mentioning poor road conditions or danger due to traffic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3 - Model 2 [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the model using more complex text features, including n-grams, character n-grams and possibly domain-specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began removing stop words using CountVectorizer since I saw that it reduced the number of features while achieving the same accuracy. I then performed grid searches using the following models to see if I can get better performance:\n",
    "\n",
    "\n",
    "1. Multinomial Naive Bayes\n",
    "2. Logistic Regression\n",
    "3. Random Forest\n",
    "4. SVM\n",
    "\n",
    "\n",
    "I searched over the following parameter grid for CountVectorizer():\n",
    "\n",
    "\n",
    "1. 'ngram_range': [(1, 1), (1, 2), (1, 5), (1, 7), (2, 3), (2, 5), (3, 8), (5, 5)]\n",
    "2. 'analyzer': ['word', 'char', 'char_wb'],\n",
    "3. 'min_df': [1, 2, 3],\n",
    "4. 'normalizer': [None, Normalizer()]\n",
    "\n",
    "Based on the results, I decided to continue with Naive Bayes and Logistic Regression since they performed the best, with similar cross-validation scores using the optimal parameters.\n",
    "\n",
    "\n",
    "The optimal CountVectorizer() parameters to use with a Naive Bayes model were:\n",
    "\n",
    "\n",
    ">{'normalizer': None, 'min_df': 3, 'analyzer': 'char_wb', 'ngram_range': (2, 5)}\n",
    "\n",
    "\n",
    "The optimal CountVectorizer() parameters to use with a Logistic Regression model were:\n",
    "\n",
    "\n",
    ">{'normalizer': None, 'ngram_range': (3, 8), 'min_df': 2, 'analyzer': 'char'}\n",
    "\n",
    "\n",
    "Next, I used tf-idf rescaling on these two models and found that they greatly reduced the accuracy of the model compared with CountVectorizer(). \n",
    "\n",
    "\n",
    "I also tried adding a feature indicating the length of the original comment string and found that it either improved or worsened the model by a negligible amount. Therefore I decided not to include that feature in my model.\n",
    "\n",
    "\n",
    "Finally, I incorporated a lemmatization function from the spaCy package to use as a custom tokenizer for CountVectorizer(). This change slightly increased my F1 macro score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T20:41:11.109062",
     "start_time": "2017-04-15T20:41:06.414832"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove stop words when vectorizing the dataset\n",
    "\n",
    "\n",
    "stop = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train = stop.fit_transform(comments_train)\n",
    "X_test = stop.transform(comments_test)\n",
    "\n",
    "lr.fit(X_train, target_train)\n",
    "stop_score = cross_val_score(lr, X_train, target_train, cv=5, scoring='f1_macro', n_jobs=8)\n",
    "\n",
    "print('Stop Word Removal Scores:', stop_score)\n",
    "print('Stop Word Removal Mean Score:', np.mean(stop_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score achieved through GridSearchCV with a Multinomial Naive Bayes model:\n",
    "    \n",
    "\n",
    ">0.525103133804\n",
    "\n",
    "\n",
    "Best parameters:\n",
    "\n",
    "\n",
    ">{'normalizer': None, 'min_df': 3, 'analyzer': 'char_wb', 'ngram_range': (2, 5)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T18:08:31.980462",
     "start_time": "2017-04-15T18:02:22.510975"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grid search using a naive Bayes model to find the best model paramaters\n",
    "\n",
    "\n",
    "params = {'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 5), (1, 7),\n",
    "                                (2, 3), (2, 5), (3, 8), (5, 5)],\n",
    "             'countvectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "           'countvectorizer__min_df': [1, 2, 3],\n",
    "           'normalizer': [None, Normalizer()]\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(make_pipeline(CountVectorizer(stop_words='english'),\n",
    "                                  Normalizer(), MultinomialNB()) ,\n",
    "                    param_grid=params, cv=5, scoring='f1_macro', n_jobs=8)\n",
    "\n",
    "nb_grid.fit(comments_train, target_train)\n",
    "print(nb_grid.best_score_)\n",
    "print(nb_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section has been commented out to prevent Travis-CI from timing out\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Best score achieved through GridSearchCV with a logistic regression model:\n",
    "\n",
    "\n",
    ">0.544554548341\n",
    "\n",
    "\n",
    "Best parameters:\n",
    "\n",
    "\n",
    ">{'normalizer': None, 'ngram_range': (3, 8), 'min_df': 2, 'analyzer': 'char', 'C': 0.1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T13:14:25.483801",
     "start_time": "2017-04-15T10:58:04.978376"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grid search using a logistic regression model to find the best model paramaters\n",
    "\n",
    "\n",
    "#lr_params = {'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 5), (1, 7),\n",
    "#                                (2, 3), (2, 5), (3, 8), (5, 5)],\n",
    "#             'countvectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "#           'countvectorizer__min_df': [1, 2, 3],\n",
    "#           'normalizer': [None, Normalizer()],\n",
    "#          'logisticregression__C': [100, 10, 1, 0.1, 0.01]\n",
    "#        }\n",
    "\n",
    "#lr_grid = GridSearchCV(make_pipeline(CountVectorizer(stop_words='english'),\n",
    "#                                    Normalizer(), LogisticRegression()), \n",
    "#                       param_grid=lr_params, cv=5, scoring='f1_macro', n_jobs=8, verbose=2)\n",
    "\n",
    "#lr_grid.fit(comments_train, target_train)\n",
    "#print(lr_grid.best_score_)\n",
    "#print(lr_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section has been commented out to prevent Travis-CI from timing out\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Best score achieved through GridSearchCV with a random forest model:\n",
    "\n",
    "\n",
    ">0.505797024798\n",
    "\n",
    "\n",
    "Best parameters:\n",
    "\n",
    "\n",
    ">{'normalizer': None, 'n_estimators': 200, 'ngram_range': (5, 5), 'analyzer': 'char', 'min_df': 3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T04:00:08.267608",
     "start_time": "2017-04-15T01:07:09.253409"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grid search using a random forest model to find the best model parameters\n",
    "\n",
    "\n",
    "#rf_params = {'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 5), (1, 7),\n",
    "#                                (2, 3), (2, 5), (3, 8), (5, 5)],\n",
    "#             'countvectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "#           'countvectorizer__min_df': [1, 2, 3],\n",
    "#           'normalizer': [None, Normalizer()],\n",
    "#             'randomforestclassifier__n_estimators': [50, 100, 150, 200],\n",
    "#        }\n",
    "\n",
    "#rf_grid = GridSearchCV(make_pipeline(CountVectorizer(stop_words='english'),\n",
    "#                                    Normalizer(), RandomForestClassifier()),\n",
    "#                      param_grid=rf_params, cv=5, scoring='f1_macro', n_jobs=8, verbose=3)\n",
    "\n",
    "#rf_grid.fit(comments_train, target_train)\n",
    "#print(rf_grid.best_score_)\n",
    "#print(rf_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-15T17:54:30.982Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Grid search using an SVM model to find the best model parameters\n",
    "\n",
    "\n",
    "#sv_params = {'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 5), (1, 7),\n",
    "#                                (2, 3), (2, 5), (3, 8), (5, 5)],\n",
    "#             'countvectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "#           'countvectorizer__min_df': [1, 2, 3],\n",
    "#           'normalizer': [None, Normalizer()],\n",
    "#          'svc__C': [100, 10, 1, 0.1, 0.01]\n",
    "#        }\n",
    "\n",
    "#sv_grid = GridSearchCV(make_pipeline(CountVectorizer(stop_words='english'),\n",
    "#                                     Normalizer(), SVC()),\n",
    "#                                    param_grid=sv_params, cv=5, scoring='f1_macro', verbose=2)\n",
    "\n",
    "#sv_grid.fit(comments_train, target_train)\n",
    "#print(sv_grid.best_score_)\n",
    "#print(sv_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T23:01:39.827571",
     "start_time": "2017-04-15T22:59:36.805086"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compare tf-idf rescaling with CountVectorizer using the best Logistic Regression model parameters\n",
    "#{'normalizer': None, 'ngram_range': (3, 8), 'min_df': 2, 'analyzer': 'char', 'C': 0.1}\n",
    "\n",
    "tf_lr_pipe = make_pipeline(TfidfVectorizer(stop_words='english', ngram_range=(3, 8),\n",
    "                                           min_df=2, analyzer='char'),LogisticRegression(C=0.1))\n",
    "\n",
    "cv_lr_pipe = make_pipeline(CountVectorizer(stop_words='english', ngram_range=(3, 8), \n",
    "                                           min_df=2, analyzer='char'), LogisticRegression(C=0.1))\n",
    "\n",
    "tf_lr_pipe.fit(comments_train, target_train)\n",
    "cv_lr_pipe.fit(comments_train, target_train)\n",
    "\n",
    "tf_score = cross_val_score(tf_lr_pipe, comments_train, target_train, cv=5, scoring='f1_macro', n_jobs=8)\n",
    "cv_score = cross_val_score(cv_lr_pipe, comments_train, target_train, cv=5, scoring='f1_macro', n_jobs=8)\n",
    "\n",
    "print('tfidf scores:', tf_score)\n",
    "print('tfidf mean score', np.mean(tf_score))\n",
    "print('CountVectorizer scores', cv_score)\n",
    "print('CountVectorizer mean score', np.mean(cv_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T18:21:44.076045",
     "start_time": "2017-04-15T18:21:25.454205"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compare tf-idf rescaling with CountVectorizer using the best Naive Bayes model parameters\n",
    "#{'normalizer': None, 'min_df': 3, 'analyzer': 'char_wb', 'ngram_range': (2, 5)}\n",
    "\n",
    "\n",
    "tf_nb_pipe = make_pipeline(TfidfVectorizer(stop_words='english', ngram_range=(2, 5), \n",
    "                                           min_df=3, analyzer='char_wb'), MultinomialNB())\n",
    "\n",
    "cv_nb_pipe = make_pipeline(CountVectorizer(stop_words='engish', ngram_range=(2, 5),\n",
    "                                          min_df=3, analyzer='char_wb'), MultinomialNB())\n",
    "\n",
    "print('tfidf:',\n",
    "      cross_val_score(tf_nb_pipe, comments_train, target_train,\n",
    "                      cv=5, scoring='f1_macro', n_jobs=8))\n",
    "print('CountVectorizer',\n",
    "     cross_val_score(cv_nb_pipe, comments_train, target_train,\n",
    "                     cv=5, scoring='f1_macro', n_jobs=8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T20:12:55.414148",
     "start_time": "2017-04-15T20:12:52.958969"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add feature that indicates the length of the original comment string\n",
    "#Evaluate this model using Naive Bayes\n",
    "\n",
    "\n",
    "nb_cv = CountVectorizer(stop_words='english', ngram_range=(2, 5),\n",
    "                       min_df=3, analyzer='char_wb')\n",
    "\n",
    "train_len = comments_train.str.len()\n",
    "train_len = np.reshape(train_len, (4833, 1))\n",
    "\n",
    "\n",
    "comments_len = nb_cv.fit_transform(comments_train)\n",
    "comments_len = hstack((comments_len, train_len))\n",
    "\n",
    "nb_score = cross_val_score(MultinomialNB(), comments_len, target_train, cv=5,\n",
    "                           scoring='f1_macro', n_jobs=8)\n",
    "\n",
    "print('nb scores:', nb_score)\n",
    "print('nb mean score:', np.mean(nb_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T20:15:41.795417",
     "start_time": "2017-04-15T20:13:44.824277"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate this model using Logistic Regression\n",
    "\n",
    "\n",
    "lr_cv = CountVectorizer(stop_words='english', ngram_range=(3, 8),\n",
    "                       min_df=2, analyzer='char')\n",
    "\n",
    "comments_len = lr_cv.fit_transform(comments_train)\n",
    "comments_len = hstack((comments_len, train_len))\n",
    "\n",
    "lr_score = cross_val_score(LogisticRegression(C=0.1), comments_len, target_train, cv=5,\n",
    "                           scoring='f1_macro', n_jobs=8)\n",
    "\n",
    "print('LR scores:', lr_score)      \n",
    "print('LR mean score:', np.mean(lr_score))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T10:07:11.713447",
     "start_time": "2017-04-16T10:07:11.659377"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Implement a custom tokenizer that uses spaCy to perform lemmatization on the comments first\n",
    "\n",
    "\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "en_nlp = spacy.load('en_default')\n",
    "\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "regexp.findall(string))\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document, entity=False, parse=False)\n",
    "    \n",
    "    return [token.lemma_ for token in doc_spacy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T22:53:40.315827",
     "start_time": "2017-04-15T22:51:11.253492"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate this model using Logistic Regression\n",
    "\n",
    "\n",
    "lr_lemma_pipe = make_pipeline(CountVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                            ngram_range=(3, 8), min_df=2, analyzer='char'), LogisticRegression(C=0.1))\n",
    "\n",
    "lr_lemma_pipe.fit(comments_train, target_train)\n",
    "lr_lemma_score = cross_val_score(lr_lemma_pipe, comments_train, target_train, cv=5,\n",
    "                                scoring='f1_macro')\n",
    "\n",
    "print('LR Lemmatization Scores:', lr_lemma_score)\n",
    "print('LR Lemmatization Mean Scores:', np.mean(lr_lemma_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T22:51:01.127735",
     "start_time": "2017-04-15T22:50:50.140064"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate this model using Naive Bayes\n",
    "\n",
    "\n",
    "nb_lemma_pipe = make_pipeline(CountVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                                              ngram_range=(2, 5), min_df=3, analyzer='char_wb'),\n",
    "                             MultinomialNB())\n",
    "\n",
    "nb_lemma_pipe.fit(comments_train, target_train)\n",
    "nb_lemma_score = cross_val_score(nb_lemma_pipe, comments_train, target_train, cv=5,\n",
    "                                scoring='f1_macro')\n",
    "\n",
    "print('NB Lemmatization Scores:', nb_lemma_score)\n",
    "print('NB Lemmatization Mean Scores:', np.mean(nb_lemma_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4 - Visualize Results [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results of the tuned model (classification results, confusion matrix, important features, example mistakes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Logistic Regression model consistently performed better than the Naive Bayes model, I decided to use that along with lemmatization going forward.\n",
    "\n",
    "\n",
    "Though the accuracy is slightly improved, this model is still making classification mistakes that are similar in nature to the ones made by the baseline model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T01:16:24.192030",
     "start_time": "2017-04-16T01:14:27.639038"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score the tuned model on the training set\n",
    "#Print the confusion matrix and classification report\n",
    "\n",
    "\n",
    "preds = cross_val_predict(lr_lemma_pipe, comments_train, target_train, cv=5)\n",
    "\n",
    "\n",
    "print(confusion_matrix(target_train, preds))\n",
    "print(classification_report(target_train, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T01:16:37.373496",
     "start_time": "2017-04-16T01:16:30.403329"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the most important character n-grams for each category\n",
    "\n",
    "\n",
    "feature_names = lr_lemma_pipe.named_steps['countvectorizer'].get_feature_names()\n",
    "\n",
    "def plot_important_features(coef, feature_names, top_n=20, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    inds = np.argsort(coef)\n",
    "    low = inds[:top_n]\n",
    "    high = inds[-top_n:]\n",
    "    important = np.hstack([low, high])\n",
    "    myrange = range(len(important))\n",
    "\n",
    "    ax.bar(myrange, coef[important])\n",
    "    ax.set_xticks(myrange)\n",
    "    ax.set_xticklabels(feature_names[important], rotation=90, ha='right')\n",
    "\n",
    "n_classes = len(lr_lemma_pipe.classes_)\n",
    "\n",
    "fig, axes = plt.subplots(n_classes, figsize=(10, 25))\n",
    "\n",
    "for ax, coef, label in zip(axes.ravel(),\n",
    "                           lr_lemma_pipe.named_steps['logisticregression'].coef_,\n",
    "                          lr_lemma_pipe.classes_):\n",
    "    #print(ax, coef, label)\n",
    "\n",
    "\n",
    "    ax.set_title(label)\n",
    "    plot_important_features(coef, np.array(feature_names), top_n=20, ax=ax)\n",
    "    \n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T01:23:16.815683",
     "start_time": "2017-04-16T01:23:16.783165"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter out the misclassified comments and show some examples\n",
    "\n",
    "\n",
    "misclassified = np.stack((np.array(comments_train), np.array(target_train), preds), axis=-1)\n",
    "misclassified = misclassified[misclassified[:, 1] != misclassified[:, 2]]\n",
    "\n",
    "\n",
    "for complaint in range(0, 2000, 400):\n",
    "    print('\\n', misclassified[complaint, 0], '\\n',\n",
    "          '\\n', 'true category:', misclassified[complaint, 1],\n",
    "          '\\n', 'misclassified as:', misclassified[complaint, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task5 - Clustering [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LDA, NMF and K-Means to the whole dataset. Can you find clusters or topics that match well with some of the ground truth labels? Use ARI to compare the methods and visualize topics and clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-16T16:35:39.551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform a grid search to find the most sensible number of clusters for K-Means clustering\n",
    "#Calculate the ARI for each iteration\n",
    "#Print the first 5 comments in each cluster\n",
    "\n",
    "\n",
    "\n",
    "cv = CountVectorizer(min_df=2, ngram_range=(1, 4), analyzer='word', stop_words='english')\n",
    "vec_comments = cv.fit_transform(comments)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "km_ari = np.zeros((20,))\n",
    "\n",
    "for n_cluster in range(20):\n",
    "    km = KMeans(n_clusters=n_cluster+3, n_jobs=8, random_state=3)\n",
    "    vec_comments = cv.fit_transform(comments)\n",
    "    km.fit(vec_comments)\n",
    "    km_preds = km.predict(vec_comments)\n",
    "    \n",
    "    km_ari[n_cluster] = adjusted_rand_score(target, km_preds)\n",
    "    \n",
    "    print('ARI:', km_ari[n_cluster])\n",
    "    print(n_cluster+3, 'bin count', np.bincount(km_preds))\n",
    "    km_clusters = np.stack((np.array(comments), km_preds), axis=-1)\n",
    "\n",
    "    for cluster in range(n_cluster+3):\n",
    "        for comment in range(5):\n",
    "            try:\n",
    "                print(cluster, '\\n', km_clusters[km_clusters[:, 1] == cluster][comment, 0], '\\n')\n",
    "            except: continue\n",
    "\n",
    "print(km_ari)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T03:03:38.429587",
     "start_time": "2017-04-16T02:51:56.783748"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first performed K-Means clustering with 14 clusters was performed on the entire dataset. I noticed that there were 5 clusters that contained only 1 data point each, so I kept reducing the number of clusters until all such clusters were gone. This was achieved when I performed K-Means clustering with only 8 clusters. I interpreted the resulting clusters in the following ways:\n",
    "\n",
    "\n",
    "1. This cluster seems to be about cars not yielding or pedestrians jaywalking and how such situations could result in accidents.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T03:03:53.018682",
     "start_time": "2017-04-16T03:03:52.581762"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T10:54:29.019737",
     "start_time": "2017-04-16T10:52:31.178991"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "cv = CountVectorizer(min_df=2, ngram_range=(1, 4), analyzer='word', stop_words='english')\n",
    "vec_comments = cv.fit_transform(comments)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "lda_ari = np.zeros((20,))\n",
    "\n",
    "for topics in range(20):\n",
    "    lda = LatentDirichletAllocation(n_topics=topics+3, learning_method='batch', n_jobs=8, random_state=3)\n",
    "    lda.fit(vec_comments)\n",
    "    lda_clusters = (np.argmax(lda.transform(vec_comments), axis=1))\n",
    "    ari = adjusted_rand_score(target, lda_clusters)\n",
    "    lda_ari[topics] = ari\n",
    "    print('number of topics:', topics+3)\n",
    "    print_top_words(lda, feature_names, 20)\n",
    "\n",
    "print(lda_ari)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T10:55:24.316568",
     "start_time": "2017-04-16T10:55:24.313066"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#6, 7, 9, 10, 12, 15 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T10:11:23.861005",
     "start_time": "2017-04-16T10:11:01.364072"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task6 - Model 3 [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the class definition for REQUESTTYPE by using the results of the clustering and results of the previous classification model. Re-assign labels using either the results of clustering or using keywords that you found during data exploration. The labels must be semantically meaningful.\n",
    "The data has a large “other” category. Apply the topic modeling and clustering techniques to this subset of the data to find possible splits of this class.\n",
    "Report accuracy using macro average f1 score (should be above .53) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add your code for task 6 here. You may use multiple cells. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit [Up to +20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a word embedding representation like word2vec for step 3 and or step 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add your code for extra credit here. You may use multiple cells. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
